{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from src.helpers.config import get_settings, Settings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_settings = get_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(docs, chunk_size=1000, chunk_overlap=50):\n",
    "    \n",
    "    # Extract text content from Document objects\n",
    "    texts = [doc.page_content for doc in docs if hasattr(doc, \"page_content\")]\n",
    "    \n",
    "    # Initialize the text splitter with the specified chunk size and overlap\n",
    "    text_splitter = NLTKTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    # Split the text into chunks and collect them in a list\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        chunks.extend(text_splitter.create_documents([text]))\n",
    "    \n",
    "    # Log the number of generated chunks for debugging\n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 735 chunks\n"
     ]
    }
   ],
   "source": [
    "file_path = \"assets/files/How Our Brain Works.pdf\"\n",
    "docs = read_pdf(file_path)\n",
    "chunks = chunk(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(app_settings.CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(app_settings.VECTOR_DB_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_key(key: str) -> str:\n",
    "    \"\"\"Encode a key to be safe for file system storage.\"\"\"\n",
    "    return hashlib.sha256(key.encode()).hexdigest()\n",
    "\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    \"\"\"Computes cosine similarity between two embeddings.\"\"\"\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(llm_model, retriever):\n",
    "    \"\"\"Creates a QA chain without chat history using the RAG approach.\"\"\"\n",
    "    system_prompt = \"\"\"You are an intelligent and versatile AI assistant designed to assist with a wide variety of tasks. \n",
    "    You excel in providing clear, concise, and accurate information, creative ideas, and thoughtful responses. \n",
    "    Use the provided context below to answer the user's query.\n",
    "    Context:\n",
    "    {context}\"\"\"\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm_model, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    \n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_vector_store(chunks, embedder, cache_dir=app_settings.VECTOR_DB_PATH):\n",
    "    \"\"\"Creates or loads a FAISS vector store with progress tracking.\"\"\"\n",
    "    if os.path.exists(cache_dir):\n",
    "        print(\"Loading existing vector store...\")\n",
    "        try:\n",
    "            # Safely load the index with explicit permission\n",
    "            return FAISS.load_local(\n",
    "                cache_dir, \n",
    "                embedder, \n",
    "                allow_dangerous_deserialization=True  # Safe for local development\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading vector store: {e}\")\n",
    "            print(\"Creating new vector store instead...\")\n",
    "    \n",
    "    print(\"Creating new vector store...\")\n",
    "    # Process chunks in batches\n",
    "    batch_size = 10\n",
    "    all_embeddings = []\n",
    "    all_texts = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Processing documents\"):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        texts = [doc.page_content for doc in batch]\n",
    "        all_texts.extend(texts)\n",
    "        embeddings = embedder.embed_documents(texts)\n",
    "        all_embeddings.extend(embeddings)\n",
    "    \n",
    "    vector_store = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(all_texts, all_embeddings)),\n",
    "        embedding=embedder\n",
    "    )\n",
    "    \n",
    "    # Save for future use\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    vector_store.save_local(cache_dir)\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, model_name, chunks, cache_dir=app_settings.VECTOR_DB_PATH):\n",
    "    \"\"\"Processes the query using RAG (Retrieval-Augmented Generation) and computes similarities.\"\"\"\n",
    "    print(\"Initializing models...\")\n",
    "    llm_model = Ollama(model=model_name, temperature=app_settings.GENERATION_TEMPERATURE)\n",
    "    embeddings_model = OllamaEmbeddings(model=model_name)\n",
    "\n",
    "    # Create cache-backed embeddings\n",
    "    print(\"Setting up embedding cache...\")\n",
    "    store = LocalFileStore(\"./assets/cache/\")\n",
    "    embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "        embeddings_model, \n",
    "        store, \n",
    "        namespace=encode_key(model_name)\n",
    "    )\n",
    "\n",
    "    # Create or load vector store\n",
    "    vector_store = create_or_load_vector_store(chunks, embedder, cache_dir)\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "    # Create QA chain and process query\n",
    "    print(\"Processing query...\")\n",
    "    rag_chain = create_qa_chain(llm_model, retriever)\n",
    "    result = rag_chain.invoke({\"input\": query})\n",
    "    \n",
    "    retrieved_docs = result[\"context\"]\n",
    "    response = result.get(\"answer\", result)\n",
    "    \n",
    "    print(\"\\nRetrieved Context:\")\n",
    "    for doc in retrieved_docs:\n",
    "        print(doc.page_content)\n",
    "    print(\"\\nGenerated Response:\\n\", response)\n",
    "\n",
    "    # Compute similarities\n",
    "    print(\"\\nComputing similarities...\")\n",
    "    query_embedding = embedder.embed_query(query)\n",
    "    response_embedding = embedder.embed_query(response)\n",
    "\n",
    "    # Process documents with progress bar\n",
    "    document_similarities = []\n",
    "    for doc in tqdm(retrieved_docs, desc=\"Analyzing documents\"):\n",
    "        if hasattr(doc, 'page_content') and doc.page_content.strip():\n",
    "            doc_embedding = embedder.embed_query(doc.page_content)\n",
    "            similarity = compute_similarity(query_embedding, doc_embedding)\n",
    "            document_similarities.append((doc.page_content, similarity))\n",
    "\n",
    "    # Sort and display results\n",
    "    document_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nDocument Similarities:\")\n",
    "    for i, (doc, similarity) in enumerate(document_similarities):\n",
    "        print(f\"Similarity {i+1}: {similarity:.4f}, Snippet: {doc[:200]}...\")\n",
    "\n",
    "    query_response_similarity = compute_similarity(query_embedding, response_embedding)\n",
    "    print(f\"\\nSimilarity between query and response: {query_response_similarity:.4f}\")\n",
    "\n",
    "    return response, document_similarities, query_response_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "Setting up embedding cache...\n",
      "Loading existing vector store...\n",
      "Processing query...\n",
      "\n",
      "Retrieved Context:\n",
      "How do you control the machine that you see when \n",
      "you look into a mirror?\n",
      "\n",
      "Your brain is a truly amazing organ.\n",
      "\n",
      "Lets \n",
      "begin the process of understanding its structure, organization, and capabilities.\n",
      "Understanding that standard \n",
      "function is a necessary prerequisite to understanding how your \n",
      "brain enables human intelligence.\n",
      "The fundamental neural functionality that allows this system to operate is the cascaded detection of synchronous patterns of input by cortical pyramidal neurons under the timing control of the thalamus.\n",
      "\n",
      "Generated Response:\n",
      " To answer your question, when you look into a mirror, you see a reflection of yourself because light from your face bounces back off the mirror's surface. The mirror doesn't have any \"machine\" that controls its behavior; it simply reflects light.\n",
      "\n",
      "However, I can provide some insights on how our brains process information and control our actions.\n",
      "\n",
      "The brain processes information through a complex network of interconnected neurons, which communicate with each other through electrical and chemical signals. Here's a simplified overview:\n",
      "\n",
      "1. **Sensory Input**: When you look into a mirror, your eyes detect the light reflected off the mirror's surface.\n",
      "2. **Signal Transmission**: The detected light is transmitted to the brain via the optic nerve, which carries visual information from the eyes to the brain.\n",
      "3. **Processing in the Brain**: The visual information is processed in various regions of the brain, including the primary visual cortex (V1), secondary visual areas (V2, V3, etc.), and higher-level areas involved in object recognition, spatial awareness, and attention.\n",
      "4. **Control Mechanisms**: Once the brain has processed the visual information, it uses control mechanisms to decide how to respond. This involves the thalamus, which acts as a relay station for sensory information, and various motor control systems that coordinate muscle movements.\n",
      "\n",
      "In terms of controlling the machine (your body) through your brain, it's essential to understand that our brains use complex neural networks to process and integrate information from multiple senses, including vision, touch, hearing, taste, and smell. This integrated information is then used to generate responses, such as moving a limb or speaking.\n",
      "\n",
      "The cascaded detection of synchronous patterns of input by cortical pyramidal neurons under the timing control of the thalamus refers to the way our brains process and integrate sensory information. Cortical pyramidal neurons are specialized cells that receive and transmit signals within the brain, while the thalamus acts as a timing controller, synchronizing neural activity across different regions.\n",
      "\n",
      "In summary, when you look into a mirror, your brain processes the visual information and uses control mechanisms to decide how to respond, which ultimately allows you to interact with the world around you.\n",
      "\n",
      "Computing similarities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing documents: 100%|██████████| 3/3 [00:02<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Similarities:\n",
      "Similarity 1: 0.6733, Snippet: Understanding that standard \n",
      "function is a necessary prerequisite to understanding how your \n",
      "brain enables human intelligence....\n",
      "Similarity 2: 0.6667, Snippet: How do you control the machine that you see when \n",
      "you look into a mirror?\n",
      "\n",
      "Your brain is a truly amazing organ.\n",
      "\n",
      "Lets \n",
      "begin the process of understanding its structure, organization, and capabilities....\n",
      "Similarity 3: 0.6419, Snippet: The fundamental neural functionality that allows this system to operate is the cascaded detection of synchronous patterns of input by cortical pyramidal neurons under the timing control of the thalamu...\n",
      "\n",
      "Similarity between query and response: 0.4711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"How does the brain process information?\"\n",
    "model_name = app_settings.GENERATION_MODEL_ID\n",
    "response, document_similarities, query_response_similarity = run_query(\n",
    "    query, model_name, chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the output we can see the similarity between the query and the retrieved documents above 66% which is good and the similarity between the query and the response is a little bit good which is above 47%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = Ollama(model=model_name, temperature=0.5)\n",
    "embeddings_model = OllamaEmbeddings(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensions: 3072\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for a sample text\n",
    "sample_text = \"test\"\n",
    "embedding_vector = embeddings_model.embed_query(sample_text)\n",
    "\n",
    "# Print the dimensions of the embedding\n",
    "print(f\"Embedding dimensions: {len(embedding_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cache-backed embeddings\n",
    "print(\"Setting up embedding cache...\")\n",
    "store = LocalFileStore(\"./assets/cache/\")\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings_model, \n",
    "    store, \n",
    "    namespace=encode_key(model_name)\n",
    ")\n",
    "\n",
    "# Create or load vector store\n",
    "vector_store = create_or_load_vector_store(chunks, embedder, cache_dir)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Create QA chain and process query\n",
    "print(\"Processing query...\")\n",
    "rag_chain = create_qa_chain(llm_model, retriever)\n",
    "result = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "retrieved_docs = result[\"context\"]\n",
    "response = result.get(\"answer\", result)\n",
    "\n",
    "print(\"\\nRetrieved Context:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n",
    "print(\"\\nGenerated Response:\\n\", response)\n",
    "\n",
    "# Compute similarities\n",
    "print(\"\\nComputing similarities...\")\n",
    "query_embedding = embedder.embed_query(query)\n",
    "response_embedding = embedder.embed_query(response)\n",
    "\n",
    "# Process documents with progress bar\n",
    "document_similarities = []\n",
    "for doc in tqdm(retrieved_docs, desc=\"Analyzing documents\"):\n",
    "    if hasattr(doc, 'page_content') and doc.page_content.strip():\n",
    "        doc_embedding = embedder.embed_query(doc.page_content)\n",
    "        similarity = compute_similarity(query_embedding, doc_embedding)\n",
    "        document_similarities.append((doc.page_content, similarity))\n",
    "\n",
    "# Sort and display results\n",
    "document_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nDocument Similarities:\")\n",
    "for i, (doc, similarity) in enumerate(document_similarities):\n",
    "    print(f\"Similarity {i+1}: {similarity:.4f}, Snippet: {doc[:200]}...\")\n",
    "\n",
    "query_response_similarity = compute_similarity(query_embedding, response_embedding)\n",
    "print(f\"\\nSimilarity between query and response: {query_response_similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
