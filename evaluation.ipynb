{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Load and parse the content of a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the PDF file to be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of Document objects containing the file's content.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(docs, chunk_size=1000, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Splits the documents into smaller chunks using NLTK-based text splitting.\n",
    "    \n",
    "    This method processes the text content from the provided Document objects and splits them into smaller\n",
    "    chunks with specified chunk size and overlap using the `NLTKTextSplitter`.\n",
    "    \n",
    "    Args:\n",
    "        docs (list): A list of Document objects containing the text to be split.\n",
    "        chunk_size (int): The maximum size of each text chunk (default is 1000).\n",
    "        chunk_overlap (int): The number of overlapping characters between consecutive chunks (default is 50).\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of Document objects representing the text chunks.\n",
    "    \"\"\"\n",
    "    # Extract text content from Document objects\n",
    "    texts = [doc.page_content for doc in docs if hasattr(doc, \"page_content\")]\n",
    "    \n",
    "    # Initialize the text splitter with the specified chunk size and overlap\n",
    "    text_splitter = NLTKTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    # Split the text into chunks and collect them in a list\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        chunks.extend(text_splitter.create_documents([text]))\n",
    "    \n",
    "    # Log the number of generated chunks for debugging\n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 735 chunks\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/marwan/marwan/Tasks/Corporatica/rag_task/streamlit/files/How Our Brain Works.pdf\"\n",
    "docs = read_pdf(file_path)\n",
    "chunks = chunk(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      " xiiiIntroduction\n",
      "The attributes of the human brain that produce human \n",
      "intelligence are the focus of this book.\n",
      "\n",
      "This book is about the real you.\n",
      "\n",
      "Not the re ﬂ ected image you see in the mirror, the \n",
      "shape of your face and body and the coloration of your skin, hair, and eyes.\n",
      "\n",
      "That’s a machine.\n",
      "\n",
      "A set of moveable connected bones driven by muscles and covered with skin and hair.\n",
      "\n",
      "You control the machine that you see in the mirror.\n",
      "\n",
      "The real you is contained within your brain.\n",
      "\n",
      "A t  t h i s  v e r y  m o m e n t ,  y o u r  b r a i n  i s  b u i l d i n g  n e u r a l  \n",
      "representations of every object you currently perceive and \n",
      "a neural representation of the space around you.\n",
      "\n",
      "These \n",
      "perceptions of objects and space, contained in different areas of your brain, are brought together to build your virtual world.\n",
      "\n",
      "That neural virtual world is your reality.\n",
      "\n",
      "The most important object in your virtual world is you.\n",
      "Embedding\n",
      ": [-0.06646905  0.03610453  0.00271671  0.01035635 -0.00708345]...\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "def create_vector_index_and_embedding_model(chunks):\n",
    "    \"\"\"\n",
    "    Creates an embedding model and vector index using Langchain and Hugging Face embeddings.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list of str): List of text chunks (documents) that need to be indexed.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Returns a tuple containing:\n",
    "            - embeddings_model: The Hugging Face embedding model used for encoding the text.\n",
    "            - vector_index: The FAISS index that stores the vectors of the documents for fast retrieval.\n",
    "            - texts: The original document texts.\n",
    "    \n",
    "    Notes:\n",
    "        - The `LocalFileStore` is used to store embeddings and cache them for future use.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up local storage for cached embeddings\n",
    "    store = LocalFileStore(\"./cache/\")  # Directory where embeddings will be cached\n",
    "    \n",
    "    # Define the embedding model ID and model arguments (device, remote code trust)\n",
    "    embed_model_id = 'intfloat/e5-small-v2'  # Hugging Face model ID for embeddings\n",
    "    model_kwargs = {\"device\": \"cpu\", \"trust_remote_code\": True}  # Parameters for the model (use CPU)\n",
    "    \n",
    "    # Create the Hugging Face embeddings model using the specified model ID and arguments\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name=embed_model_id, model_kwargs=model_kwargs)\n",
    "    \n",
    "    # Wrap the Hugging Face model with caching to avoid recalculating embeddings\n",
    "    embedder = CacheBackedEmbeddings.from_bytes_store(embeddings_model, store, namespace=embed_model_id)\n",
    "    \n",
    "    # Use FAISS to create a vector index from the documents (chunks)\n",
    "    vector_index = FAISS.from_documents(chunks, embedder)  # Generate the index based on the document embeddings\n",
    "    \n",
    "    # Return the embeddings model, vector index, and the original texts\n",
    "    return embeddings_model, vector_index, chunks\n",
    "\n",
    "# Retrieve text and embeddings from FAISS\n",
    "def retrieve_text_and_embeddings(vector_index, texts):\n",
    "    \"\"\"\n",
    "    Retrieves the list of text and embeddings from the FAISS vector index.\n",
    "    \n",
    "    Args:\n",
    "        vector_index (FAISS): The FAISS vector index object.\n",
    "        texts (list of str): The original documents (text chunks).\n",
    "        \n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains a text chunk and its corresponding embedding vector.\n",
    "    \"\"\"\n",
    "    # Accessing the FAISS index directly to get the embeddings and their corresponding IDs\n",
    "    embeddings = vector_index.index.reconstruct_n(0, len(texts))  # Get all embeddings\n",
    "    text_and_embeddings = [(texts[i], embeddings[i]) for i in range(len(texts))]\n",
    "    \n",
    "    return text_and_embeddings\n",
    "\n",
    "# Create the embedding model and vector index\n",
    "embeddings_model, vector_index, texts = create_vector_index_and_embedding_model(chunks)\n",
    "\n",
    "# Retrieve the list of texts and their embeddings\n",
    "text_and_embeddings = retrieve_text_and_embeddings(vector_index, texts)\n",
    "\n",
    "# Print the results\n",
    "for text, embedding in text_and_embeddings:\n",
    "    print(f\"Text:\\n {text.page_content}\")\n",
    "    print(f\"Embedding\\n: {embedding[:5]}...\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
